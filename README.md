<div align=center class="logo">
      <img src="Repo_image/DiffTSR_icon.png" style="width:640px">
   </a>
</div>

#
Diffusion-based Blind Text Image Super-Resolution (CVPR2024)
<a href='https://arxiv.org/abs/2312.08886'><img src='https://img.shields.io/badge/arXiv-2312.08886-b31b1b.svg'></a> &nbsp;&nbsp;

[Yuzhe Zhang](https://yuzhezhang-1999.github.io/)<sup>1</sup> | [Jiawei Zhang](https://scholar.google.com/citations?user=0GTpIAIAAAAJ)<sup>2</sup> | Hao Li<sup>2</sup> | [Zhouxia Wang](https://scholar.google.com/citations?user=JWds_bQAAAAJ)<sup>3</sup> | Luwei Hou<sup>2</sup> | [Dongqing Zou](https://scholar.google.com/citations?user=K1-PFhYAAAAJ)<sup>2</sup> | [Liheng Bian](https://scholar.google.com/citations?user=66IFMDEAAAAJ)<sup>1</sup>

<sup>1</sup>Beijing Institute of Technology, <sup>2</sup>SenseTime Research, <sup>3</sup>The University of Hong Kong

## ğŸ’¬ Q&A
Please Read Before Tryingï½

### ğŸ‡¨ğŸ‡³ ä¸­æ–‡Q&Aï¼šå¯¹äºå¤§å®¶å…³å¿ƒçš„ä¸€äº›ç»†èŠ‚é—®é¢˜ï¼Œè¿™é‡Œç»™å‡ºäº†å½’çº³ä¾›å¤§å®¶å‚è€ƒ

1. Q: IDMä¸­Unetç”¨çš„æ˜¯Stable-Diffusionçš„æƒé‡å—?

   A: ä¸æ˜¯ã€‚IDMçš„Unetæ˜¯ä»å¤´è®­ç»ƒçš„ï¼Œæ²¡æœ‰åŠ è½½ä»»ä½•é¢„è®­ç»ƒæƒé‡ï¼ŒIDMçš„ç»“æ„ä¹Ÿå’Œä»»ä½•ä¸€ä¸ªDiffusionæ¨¡å‹çš„Unetä¸ä¸€è‡´ã€‚ä½†æ˜¯VAEæ˜¯åŠ è½½äº†ldmçš„f4çš„VAEåœ¨open-imageä¸Šé¢„è®­ç»ƒçš„æƒé‡ï¼Œç„¶ååœ¨æœ¬é¡¹ç›®çš„CTW-HQ-Trainæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œå¾®è°ƒäº†100,000iterï¼Œbatch_size=16ã€‚æ­¤å¤–åŒ…æ‹¬TDMå’ŒMoMåœ¨å†…çš„æ¨¡å‹å‡æœªä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œå‡ä¸ºä»å¤´è®­ç»ƒè·å¾—ã€‚è¯¦ç»†è®­ç»ƒè®¾ç½®è¯·çœ‹ [é™„åŠ ææ–™](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Zhang_Diffusion-based_Blind_Text_CVPR_2024_supplemental.pdf)Section 1.4ã€‚

2. Q: DiffTSRæ¨¡å‹çš„è¾“å…¥å°ºå¯¸å’Œè¦æ±‚ï¼Œéœ€è¦å°†è¾“å…¥resizeå—ï¼Ÿ

   A: æ¨¡å‹çš„LRè¾“å…¥éœ€è¦ç»Ÿä¸€resizeåˆ° width=512/height=128ï¼›æ­¤å¤–å› ä¸ºæœ¬é¡¹ç›®ä»…è€ƒè™‘å•è¡Œæ–‡æœ¬è¾“å…¥ï¼Œæ‚¨æ‰€è¾“å…¥çš„å›¾ç‰‡éœ€è¦åªåŒ…å«ä¸€è¡Œæ–‡æœ¬ï¼Œå¯¹äºIDMå’ŒTDMéƒ½ä»…é€‚é…äº†å•è¡Œæ–‡æœ¬ï¼Œåœ¨å¤šè¡Œæœ¬æ–‡è¾“å…¥ä¼šå‡ºç°æ•ˆæœæ‰­æ›²å’Œé”™è¯¯çš„ç»“æœã€‚

3. Q: å›¾ç‰‡çš„æ¨ç†é€Ÿåº¦éå¸¸æ…¢ï¼Œæœ‰ä»€ä¹ˆè§£å†³åŠæ³•å—ï¼Ÿ
   
   A: å› ä¸ºè¯¥é¡¹ç›®çš„æŠ€æœ¯åŸºäºDiffusionï¼Œæ‰€ä»¥æ¯å¤„ç†ä¸€å¼ å›¾åƒéƒ½éœ€è¦è¿›è¡ŒTæ¬¡è¿­ä»£è¿‡ç¨‹ï¼ˆT=200é»˜è®¤ï¼‰ã€‚è‹¥æƒ³æå‡æ¨ç†é€Ÿåº¦ï¼Œå¯ä»¥è€ƒè™‘ï¼š
   ```
   ï¼ˆ1ï¼‰å‡å°Tï¼Œå› ä¸ºé‡‡æ ·å™¨ä¸ºDDIMï¼Œåœ¨T=20æ—¶ä»æœ‰è¾ƒå¥½è¡¨ç°ï¼›
   ï¼ˆ2ï¼‰å¯¹DiffTSRæ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œå¯ä»¥å‚è€ƒDiffusionæ¨¡å‹é‡åŒ–çš„ç›¸å…³Repoï¼›
   ï¼ˆ3ï¼‰ä½¿ç”¨æœ¬é¡¹ç›®çš„Baseline modelï¼Œè™½ç„¶Baselineä¼šåœ¨ä¸€å®šç¨‹åº¦ä¸Šå‡å°æ€§èƒ½ï¼Œä½†æ˜¯å¯ä»¥æå‡çº¦2å€çš„è€—æ—¶æ”¶ç›Šï¼Œä¸”å¯¹äºå¤§å¤šæ•°åœºæ™¯ä¸ä¼šæœ‰æ˜æ˜¾æ•ˆæœé€€åŒ–;
   ï¼ˆ4ï¼‰å¯¹æˆ‘çš„æ¨¡å‹è¿›è¡Œè’¸é¦ï¼›æˆ–è€…æ ¹æ®è¯¥è®ºæ–‡è®­ç»ƒä¸€ä¸ªæ›´å°çš„IDMæ¨¡å‹ï¼Œæ–‡å­—åœºæ™¯å¯èƒ½å¹¶ä¸éœ€è¦ç±»ä¼¼äºé€šç”¨åœºæ™¯å›¾åƒç”Ÿæˆè¿™ä¹ˆé‡çš„æ¨¡å‹ã€‚
   ```

4. Q: åœ¨è®­ç»ƒIDMçš„æ—¶å€™æŸå¤±æ˜¯æ€ä¹ˆè®¾ç½®çš„ï¼Œtext_recognization lossæ˜¯å¦‚ä½•å®ç°çš„ï¼Ÿ

   A: è®­ç»ƒIDMçš„æ—¶å€™ï¼Œä½¿ç”¨äº†ä¸¤ä¸ªæŸå¤±å‡½æ•°ï¼ˆ1ï¼‰L2 lossç”¨äºé¢„æµ‹å™ªå£°ï¼Œï¼ˆ2ï¼‰OCR lossç”¨äºä»é¢„æµ‹å‡ºçš„å¹²å‡€çš„X0ä¸Šæ£€æµ‹æ–‡å­—ã€‚
   ```
    (1) å…¶ä¸­ L2 loss å³ä¼ ç»Ÿdiffsionæ¨¡å‹ä¸­æ‰€ç”¨çš„ç”¨äºæœ€å°åŒ–ï¼ˆUnetè¾“å‡º-noise mapï¼‰,ä»è€Œä½¿Unetå…·å¤‡å™ªå£°ä¼°è®¡èƒ½åŠ›ï¼›
    (2) å…¶ä¸­ OCR loss ä¸ºä»z_tè·å¾—z^(t-1)åï¼Œå†æ ¹æ®è¿™ä¸ªå¾—åˆ°z^0ï¼Œç„¶åè·å¾—x^0=Decoder(z^0)ï¼Œç„¶åå°†x^0è¾“å…¥ç»™æƒé‡å†»ç»“çš„TransOCRæ¨¡å‹ä»è€Œè·å¾—x^0ä¸Šæ‰€åŒ…å«çš„æ–‡å­—embeddingï¼Œç„¶åè®¡ç®—é¢„æµ‹çš„pred-text-embeddingå’Œgt-text-embeddingä¹‹é—´çš„cross-entropy lossï¼Œå³ä¸ºè¯¥OCR lossï¼Œä¸”OCR lossæ·»åŠ äº†ä¸€ä¸ªweight=0.02çš„çº¦æŸã€‚
   ```
   å†…å®¹è¯¦è§[issue](https://github.com/YuzheZhang-1999/DiffTSR/issues/13)ã€‚

5. Q: è®­ç»ƒçš„æŸå¤±å‡½æ•°æ˜¯ä»€ä¹ˆï¼Ÿ
  
   A: DiffTSRæ¨¡å‹çš„è®­ç»ƒç»å†äº†ä¸‰ä¸ªæ­¥éª¤ï¼Œæ¯ä¸ªæ­¥éª¤ç”¨äº†ä¸åŒçš„æŸå¤±å‡½æ•°çš„ç»„åˆã€‚åˆ†åˆ«ä¸ºï¼šè®­ç»ƒIDMã€è®­ç»ƒTDMã€è®­ç»ƒMoMã€‚
   ```
    (1) è®­ç»ƒIDMï¼ŒIDMä»å¤´è®­ç»ƒUnetï¼Œè¯¥æŸå¤±ä¸ºL_IDMï¼ŒåŒ…å«L2 losså’ŒOCR lossï¼›
    (2) è®­ç»ƒTDMï¼ŒTDMä»å¤´è®­ç»ƒTransformerï¼Œè¯¥æŸå¤±ä¸ºL_TDMï¼Œå‚è€ƒ[Multinomial DIffusion](https://arxiv.org/pdf/2102.05379)Section 4;
    (3) è®­ç»ƒæ•´ä¸ªDiffTSRï¼Œå†»ç»“IDMå’ŒTDMï¼Œä»…è®­ç»ƒMoMï¼Œè¯¥æŸå¤±ä¸ºL_MoM = L_IDM+L_TDM*weight;
   ```
   å…¶ä¸­ï¼š
   
   $$
    L_{IDM} = L_2(\epsilon, \epsilon_{pred, t}) + L_{ocr}(Transocr(x^{pred}_0), TextEmbed_{gt})*\lambda, \lambda=0.02
   $$

   $$
    L_{TDM} = KL(\mathcal{C(\pi_{post}(\mathbf{c_t}, \mathbf{c_0}))} || \mathcal{C(\pi_{post}(\mathbf{c_t}, \mathbf{c_{pred, t}}))})
   $$

   $$
    L_{MoM} = L_{IDM} + L_{TDM}*\lambda, \lambda=1
   $$

   å…·ä½“ç¬¦å·å®šä¹‰å’ŒåŸç†æ¨å¯¼è¯¦è§[é™„åŠ ææ–™](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Zhang_Diffusion-based_Blind_Text_CVPR_2024_supplemental.pdf)Section1å’ŒAlgorithm 1 DiffTSR Trainingã€‚

    æœªå®Œå¾…ç»­...

</details>

<details>
<summary>ğŸ‡¬ğŸ‡§ English Version(Click to expand)</summary>

    Pending...

</details>


## ğŸ“¢ News
- **2024.05** ğŸš€Inference code has been released, enjoy.
- **2024.04** ğŸš€Official repository of DiffTSR.
- **2024.03** ğŸŒŸThe implementation code will be released shortly.
- **2024.03** â¤ï¸Accepted by CVPR2024.

## ğŸ”¥ TODO
- [x] Attach the detailed implementation and supplementary material.
- [x] Add inference code and checkpoints for blind text image SR.
- [ ] Add training code and scripts.

## ğŸ‘ï¸ Gallery

[<img src="Repo_image/ImgSli_1.jpg" width="256px"/>](https://imgsli.com/MjY0MTk5) [<img src="Repo_image/ImgSli_2.jpg" width="256px"/>](https://imgsli.com/MjY0MjA0)

## ğŸ› ï¸ Try
### Dependencies and Installation

- Pytorch >= 1.7.0
- CUDA >= 11.0
```
# git clone this repository
git clone https://github.com/YuzheZhang-1999/DiffTSR
cd DiffTSR

# create new anaconda env
conda env create -f environment.yaml
conda activate DiffTSR
```
### Download the checkpoint
Please download the checkpoint file from the URL below to the ./ckpt/ folder.

- [[GoogleDrive](https://drive.google.com/drive/folders/1K6k5ZcvF3w-1MDN_gXQTdsLgFZ2SM8qy?usp=drive_link)] 

- [[BaiduDisk](https://pan.baidu.com/s/1hfaQzIp_V6H8AhAq5dfr8A)] [Password: vk9n] 




### Inference
```
python inference_DiffTSR.py
# check the code for more detail
```

## ğŸ” Overview of DiffTSR
![DiffTSR](Repo_image/paper-DiffTSR-model.jpg)
### Abstract
Recovering degraded low-resolution text images is challenging, especially for Chinese text images with complex strokes and severe degradation in real-world scenarios.
Ensuring both text fidelity and style realness is crucial for high-quality text image super-resolution.
Recently, diffusion models have achieved great success in natural image synthesis and restoration due to their powerful data distribution modeling abilities and data generation capabilities
In this work, we propose an Image Diffusion Model (IDM) to restore text images with realistic styles.
For diffusion models, they are not only suitable for modeling realistic image distribution but also appropriate for learning text distribution.
Since text prior is important to guarantee the correctness of the restored text structure according to existing arts, we also propose a Text Diffusion Model (TDM) for text recognition which can guide IDM to generate text images with correct structures.
We further propose a Mixture of  Multi-modality module (MoM) to make these two diffusion models cooperate with each other in all the diffusion steps.
Extensive experiments on synthetic and real-world datasets demonstrate that our Diffusion-based Blind Text Image Super-Resolution (DiffTSR) can restore text images with more accurate text structures as well as more realistic appearances simultaneously.

### Visual performance comparison overview 
![DiffTSR](Repo_image/paper-fig1.jpg)
Blind text image super-resolution results between different methods on synthetic and real-world text images. Our method can restore text images with high text fidelity and style realness under complex strokes, severe degradation, and various text styles.


<details>
  <summary>ğŸ“· More Visual Results</summary>

  ## ![DiffTSR](Repo_image/paper-visual-comp-1.jpg)
  ## ![DiffTSR](Repo_image/paper-visual-comp-2.jpg)
  ## ![DiffTSR](Repo_image/paper-visual-comp-3.jpg)
  ## ![DiffTSR](Repo_image/paper-visual-comp-4.jpg)
  ## ![DiffTSR](Repo_image/paper-visual-comp-5.jpg)
  ## ![DiffTSR](Repo_image/paper-visual-comp-6.jpg)

</details>


## ğŸ“Citations
```
@inproceedings{zhang2024diffusion,
  title={Diffusion-based Blind Text Image Super-Resolution},
  author={Zhang, Yuzhe and Zhang, Jiawei and Li, Hao and Wang, Zhouxia and Hou, Luwei and Zou, Dongqing and Bian, Liheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={25827--25836},
  year={2024}
}
```

## ğŸ« License
This project is released under the [Apache 2.0 license](LICENSE).

## Acknowledgement
Thanks to these awesome workï¼š
- [Latent Diffusion](https://github.com/CompVis/latent-diffusion)
- [Benchmarking Chinese Text Recognition](https://github.com/FudanVI/benchmarking-chinese-text-recognition)

<details>
<summary>Statistics</summary>

![visitors](https://visitor-badge.laobi.icu/badge?page_id=YuzheZhang-1999/DiffTSR)

</details>