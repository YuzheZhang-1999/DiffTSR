<div align=center class="logo">
      <img src="Repo_image/DiffTSR_icon.png" style="width:640px">
   </a>
</div>

#
Diffusion-based Blind Text Image Super-Resolution (CVPR2024)
<a href='https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Diffusion-based_Blind_Text_Image_Super-Resolution_CVPR_2024_paper.pdf'><img src='https://img.shields.io/badge/CVPR-2024-blue.svg'></a> &nbsp;&nbsp;
<a href='https://openaccess.thecvf.com/content/CVPR2024/supplemental/Zhang_Diffusion-based_Blind_Text_CVPR_2024_supplemental.pdf'><img src='https://img.shields.io/badge/Supplementary-Material-9cf'></a> &nbsp;&nbsp;

[Yuzhe Zhang](https://yuzhezhang-1999.github.io/)<sup>1</sup> | [Jiawei Zhang](https://scholar.google.com/citations?user=0GTpIAIAAAAJ)<sup>2</sup> | Hao Li<sup>2</sup> | [Zhouxia Wang](https://scholar.google.com/citations?user=JWds_bQAAAAJ)<sup>3</sup> | Luwei Hou<sup>2</sup> | [Dongqing Zou](https://scholar.google.com/citations?user=K1-PFhYAAAAJ)<sup>2</sup> | [Liheng Bian](https://scholar.google.com/citations?user=66IFMDEAAAAJ)<sup>1</sup>

<sup>1</sup>Beijing Institute of Technology, <sup>2</sup>SenseTime Research, <sup>3</sup>The University of Hong Kong

## ğŸ’¬ Q&A
Please Read Before Trying.

<details>
<summary> ğŸ‡¨ğŸ‡³ ä¸­æ–‡ Q&Aï¼šå¯¹äºå¤§å®¶å…³å¿ƒçš„ä¸€äº›ç»†èŠ‚é—®é¢˜ï¼Œè¿™é‡Œè¿›è¡Œäº†å½’çº³ä¾›å¤§å®¶å‚è€ƒ (ç‚¹å‡»å±•å¼€) </summary>

0. **DiffTSRçš„å¯¹çœŸå®ä¸–ç•Œå›¾ç‰‡çš„æ³›åŒ–æ€§ï¼Œæ˜¯å¦æ³›åŒ–åˆ°Real-World Scenariosï¼Ÿ**

   **A:** DiffTSRåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è€ƒè™‘åˆ°äº†çœŸå®ä¸–ç•Œçš„å„ç§é€€åŒ–ï¼Œç»§æ‰¿äº†BSRGANå’ŒReal-ESRGANä¸­å¯¹äºå¤æ‚é€€åŒ–æµç¨‹çš„æ„å»ºã€‚ä¸”â€œBlind Text Image Super-Resolutionâ€çš„Blind-ç›²å›¾åƒæ¢å¤å°±æ˜¯æŒ‡é’ˆå¯¹çœŸå®ä¸–ç•ŒæœªçŸ¥é€€åŒ–çš„å›¾åƒæ¢å¤ã€‚


1. **IDM ä¸­ Unet ç”¨çš„æ˜¯ Stable-Diffusion çš„æƒé‡å—ï¼Ÿ**

   **A:** ä¸æ˜¯ã€‚IDM çš„ Unet æ˜¯ä»å¤´è®­ç»ƒçš„ï¼Œæ²¡æœ‰åŠ è½½ä»»ä½•é¢„è®­ç»ƒæƒé‡ï¼ŒIDM çš„ç»“æ„ä¹Ÿå’Œä»»ä½•ä¸€ä¸ª Diffusion æ¨¡å‹çš„ Unet ä¸ä¸€è‡´ã€‚ä½†æ˜¯ VAE æ˜¯åŠ è½½äº† ldm çš„ f4 VAE åœ¨ Open-Image ä¸Šé¢„è®­ç»ƒçš„æƒé‡ï¼Œç„¶ååœ¨æœ¬é¡¹ç›®çš„ CTR-TSR-Train æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œå¾®è°ƒäº† 100,000 iterï¼Œbatch_size=16ã€‚æ­¤å¤–ï¼ŒåŒ…æ‹¬ TDM å’Œ MoM åœ¨å†…çš„æ¨¡å‹å‡æœªä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œå‡ä¸ºä»å¤´è®­ç»ƒè·å¾—ã€‚è¯¦ç»†è®­ç»ƒè®¾ç½®è¯·çœ‹ [é™„åŠ ææ–™](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Zhang_Diffusion-based_Blind_Text_CVPR_2024_supplemental.pdf) Section 1.4ã€‚

2. **DiffTSR æ¨¡å‹çš„è¾“å…¥å°ºå¯¸å’Œè¦æ±‚ï¼Œéœ€è¦å°†è¾“å…¥ resize å—ï¼Ÿ**

   **A:** æ¨¡å‹çš„ LR è¾“å…¥éœ€è¦ç»Ÿä¸€ resize åˆ° `width=512` / `height=128`ã€‚æ­¤å¤–ï¼Œå› ä¸ºæœ¬é¡¹ç›®ä»…è€ƒè™‘å•è¡Œæ–‡æœ¬è¾“å…¥ï¼Œè¾“å…¥å›¾ç‰‡éœ€è¦åªåŒ…å«ä¸€è¡Œæ–‡æœ¬ã€‚IDM å’Œ TDM ä»…é€‚é…å•è¡Œæ–‡æœ¬ï¼Œå¤šè¡Œæ–‡æœ¬è¾“å…¥ä¼šå¯¼è‡´æ•ˆæœæ‰­æ›²å’Œé”™è¯¯çš„ç»“æœã€‚

3. **å›¾ç‰‡çš„æ¨ç†é€Ÿåº¦éå¸¸æ…¢ï¼Œæœ‰ä»€ä¹ˆè§£å†³åŠæ³•å—ï¼Ÿ**

   **A:** ç”±äºæœ¬é¡¹ç›®åŸºäº Diffusion æŠ€æœ¯ï¼Œæ¯å¤„ç†ä¸€å¼ å›¾åƒéƒ½éœ€è¦è¿›è¡Œ `T` æ¬¡è¿­ä»£ï¼ˆé»˜è®¤ `T=200`ï¼‰ã€‚è‹¥æƒ³æå‡æ¨ç†é€Ÿåº¦ï¼Œå¯ä»¥è€ƒè™‘ï¼š
   
   - **å‡å° `T`**ï¼Œç”±äºé‡‡æ ·å™¨ä¸º DDIMï¼Œåœ¨ `T=20` æ—¶ä»æœ‰è¾ƒå¥½è¡¨ç°ã€‚
   - **å¯¹ DiffTSR æ¨¡å‹è¿›è¡Œé‡åŒ–**ï¼Œå¯å‚è€ƒ Diffusion æ¨¡å‹é‡åŒ–çš„ç›¸å…³ Repoã€‚
   - **ä½¿ç”¨æœ¬é¡¹ç›®çš„ Baseline model**ï¼Œè™½ç„¶ Baseline ä¼šåœ¨ä¸€å®šç¨‹åº¦ä¸Šé™ä½æ€§èƒ½ï¼Œä½†å¯æå‡çº¦ 2 å€çš„æ¨ç†é€Ÿåº¦ï¼Œå¹¶ä¸”åœ¨å¤§å¤šæ•°åœºæ™¯ä¸‹ä¸ä¼šæ˜æ˜¾é€€åŒ–ã€‚
   - **å¯¹æ¨¡å‹è¿›è¡Œè’¸é¦**ï¼Œæˆ–åŸºäºè®ºæ–‡è®­ç»ƒä¸€ä¸ªæ›´å°çš„ IDM æ¨¡å‹ï¼Œæ–‡æœ¬åœºæ™¯å¯èƒ½ä¸éœ€è¦åƒé€šç”¨åœºæ™¯å›¾åƒç”Ÿæˆé‚£æ ·é‡çš„æ¨¡å‹ã€‚

4. **åœ¨è®­ç»ƒ IDM æ—¶ï¼ŒæŸå¤±æ˜¯å¦‚ä½•è®¾ç½®çš„ï¼Ÿtext_recognition loss æ˜¯å¦‚ä½•å®ç°çš„ï¼Ÿ**

   **A:** è®­ç»ƒ IDM æ—¶ä½¿ç”¨äº†ä¸¤ä¸ªæŸå¤±å‡½æ•°ï¼š
   
   - **L2 loss**ï¼šç”¨äºé¢„æµ‹å™ªå£°ã€‚
   - **OCR loss**ï¼šç”¨äºä»é¢„æµ‹å‡ºçš„å¹²å‡€ `x^0` ä¸Šæ£€æµ‹æ–‡å­—ã€‚
   
   å…·ä½“æ¥è¯´ï¼š
   - L2 loss æ˜¯ä¼ ç»Ÿ diffusion æ¨¡å‹ä¸­ç”¨äºæœ€å°åŒ– `(Unet è¾“å‡º - noise map)`ï¼Œä½¿ Unet å…·å¤‡å™ªå£°ä¼°è®¡èƒ½åŠ›ã€‚
   - OCR loss é€šè¿‡ `z_t` è®¡ç®— `z^(t-1)`ï¼Œå†å¾—åˆ° `z^0`ï¼Œç„¶åè§£ç  `z^0` å¾—åˆ° `x^0`ã€‚å°† `x^0` è¾“å…¥å†»ç»“æƒé‡çš„ TransOCR æ¨¡å‹ï¼Œè·å¾— `x^0` ä¸Šçš„æ–‡å­— embeddingï¼Œè®¡ç®—é¢„æµ‹çš„ `pred-text-embedding` å’Œ `gt-text-embedding` ä¹‹é—´çš„ cross-entropy lossï¼ŒOCR loss é¢å¤–æ·»åŠ äº† `weight=0.02` çº¦æŸã€‚
   
   è¯¦ç»†å†…å®¹å‚è§ [Issue](https://github.com/YuzheZhang-1999/DiffTSR/issues/13)ã€‚

5. **è®­ç»ƒçš„æŸå¤±å‡½æ•°æ˜¯ä»€ä¹ˆï¼Ÿ**

   **A:** DiffTSR æ¨¡å‹è®­ç»ƒç»å†äº†ä¸‰ä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µä½¿ç”¨äº†ä¸åŒæŸå¤±å‡½æ•°çš„ç»„åˆï¼š
   
   - **è®­ç»ƒ IDM**ï¼šIDM ä»å¤´è®­ç»ƒ Unetï¼ŒæŸå¤±å‡½æ•°ä¸º `L_IDM`ï¼ŒåŒ…å« `L2 loss` å’Œ `OCR loss`ã€‚
   - **è®­ç»ƒ TDM**ï¼šTDM ä»å¤´è®­ç»ƒ Transformerï¼ŒæŸå¤±å‡½æ•°ä¸º `L_TDM`ï¼Œå‚è€ƒ [Multinomial Diffusion](https://arxiv.org/pdf/2102.05379) Section 4ã€‚
   - **è®­ç»ƒ DiffTSR æ•´ä½“**ï¼šå†»ç»“ IDM å’Œ TDMï¼Œä»…è®­ç»ƒ MoMï¼ŒæŸå¤±å‡½æ•°ä¸º `L_MoM = L_IDM + L_TDM * weight`ã€‚
   
   å…¶ä¸­ï¼š
   
   $$
   L_{IDM} = L_2 + \lambda*L_{OCR}, \lambda=0.02
   $$

   $$
   L_{TDM} = KL(\mathcal{C(\pi_{post}(\mathbf{c_t}, \mathbf{c_0}))} || \mathcal{C(\pi_{post}(\mathbf{c_t}, \mathbf{c_{pred, t}}))})
   $$

   $$
   L_{MoM} = L_{IDM} + \lambda*L_{TDM}, \lambda=1
   $$

   å…·ä½“ç¬¦å·å®šä¹‰å’Œç†è®ºæ¨å¯¼è¯¦è§ [é™„åŠ ææ–™](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Zhang_Diffusion-based_Blind_Text_CVPR_2024_supplemental.pdf) Section 1 åŠ Algorithm 1 DiffTSR Trainingã€‚

   **æœªå®Œå¾…ç»­...**

</details>

<details>
<summary>ğŸ‡¬ğŸ‡§ English Q&A: For some details you may want to know, here is a summary for your reference (click to expand)</summary>

0. **Generalization of DiffTSR to Real-World Scenarios**

   **A:** DiffTSR takes various real-world degradations into account during training, inheriting the complex degradation modeling from BSRGAN and Real-ESRGAN. Moreover, the "Blind" in "Blind Text Image Super-Resolution" specifically refers to the restoration of images with unknown degradations, which is targeted at real-world scenarios.

1. **Does the Unet in IDM use Stable-Diffusion weights?**

   **A:** No. The Unet in IDM is trained from scratch and does not load any pre-trained weights. Additionally, the structure of IDM is different from any Diffusion model's Unet. However, the VAE loads the pre-trained weights from `ldm f4 VAE`, which was pre-trained on the Open-Image dataset and then fine-tuned on the CTR-TSR-Train dataset in this project. The fine-tuning was conducted for 100,000 iterations with a batch size of 16. Moreover, models including TDM and MoM were also trained from scratch without using any pre-trained models. For detailed training settings, please refer to [Supplementary Material](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Zhang_Diffusion-based_Blind_Text_CVPR_2024_supplemental.pdf) Section 1.4.

2. **What are the input size and requirements for the DiffTSR model? Does the input need to be resized?**

   **A:** The LR input of the model needs to be uniformly resized to `width=512` and `height=128`. Additionally, since this project only considers single-line text input, the input image must contain only one line of text. Both IDM and TDM are designed specifically for single-line text, and multi-line text input will result in distortion and incorrect results.

3. **The inference speed of the image is very slow. What are the possible solutions?**

   **A:** Since this project is based on Diffusion technology, processing a single image requires `T` iterations (default `T=200`). To improve inference speed, you may consider:
   
   - **Reducing `T`**, as the sampler is DDIM, and it still performs well at `T=20`.
   - **Quantizing the DiffTSR model**, referring to relevant repositories on Diffusion model quantization.
   - **Using the project's Baseline model**, which, although it may slightly reduce performance, provides approximately 2Ã— speed-up while maintaining acceptable performance in most scenarios.
   - **Performing model distillation on IDM** or training a smaller IDM model. In textual scenarios, a heavy model like general image generation may not be necessary.

4. **How is the loss function set when training IDM? How is the text recognition loss implemented?**

   **A:** When training IDM, two loss functions are used:
   
   - **L2 loss**: Used for predicting noise.
   - **OCR loss**: Used for detecting text from the predicted clean `x^0`.
   
   Specifically:
   - **L2 loss** is the traditional loss used in diffusion models, minimizing the difference between Unet output and noise map, enabling Unet to estimate noise.
   - **OCR loss** is computed by first obtaining `z^(t-1)` from `z_t`, then deriving `z^0`, and subsequently decoding `z^0` to obtain `x^0`. The decoded `x^0` is fed into a frozen TransOCR model to obtain the text embedding in `x^0`. The cross-entropy loss is then computed between the predicted text embedding (`pred-text-embedding`) and the ground truth text embedding (`gt-text-embedding`). A weight constraint of `weight=0.02` is applied to the OCR loss.
   
   For more details, see [Issue](https://github.com/YuzheZhang-1999/DiffTSR/issues/13).

5. **What are the loss functions used during training?**

   **A:** The DiffTSR model training consists of three stages, each using a different combination of loss functions:
   
   - **Training IDM**: IDM trains Unet from scratch using loss `L_IDM`, which includes L2 loss and OCR loss.
   - **Training TDM**: TDM trains the Transformer from scratch using loss `L_TDM`, referring to [Multinomial Diffusion](https://arxiv.org/pdf/2102.05379) Section 4.
   - **Training the entire DiffTSR**: IDM and TDM are frozen, and only MoM is trained with loss `L_MoM = L_IDM + L_TDM * weight`.
   
   Where:
   
   $$
   L_{IDM} = L_2 + \lambda*L_{OCR}, \lambda=0.02
   $$

   $$
   L_{TDM} = KL(\mathcal{C(\pi_{post}(\mathbf{c_t}, \mathbf{c_0}))} || \mathcal{C(\pi_{post}(\mathbf{c_t}, \mathbf{c_{pred, t}}))})
   $$

   $$
   L_{MoM} = L_{IDM} + \lambda*L_{TDM}, \lambda=1
   $$

   For detailed symbol definitions and theoretical derivations, see [Supplementary Material](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Zhang_Diffusion-based_Blind_Text_CVPR_2024_supplemental.pdf) Section 1 and Algorithm 1 DiffTSR Training.

   **To be continued...**


</details>


## ğŸ“¢ News
- ğŸš€Training Code has been released, enjoy.
- **2024.05** ğŸš€Inference code has been released, enjoy.
- **2024.04** ğŸš€Official repository of DiffTSR.
- **2024.03** ğŸŒŸThe implementation code will be released shortly.
- **2024.03** â¤ï¸Accepted by CVPR2024.

## ğŸ”¥ TODO
- [x] Attach the detailed implementation and supplementary material.
- [x] Add inference code and checkpoints for blind text image SR.
- [x] Add training code and scripts.

## ğŸ‘ï¸ Gallery

[<img src="Repo_image/ImgSli_1.jpg" width="256px"/>](https://imgsli.com/MjY0MTk5) [<img src="Repo_image/ImgSli_2.jpg" width="256px"/>](https://imgsli.com/MjY0MjA0)

## ğŸ› ï¸ Try
### Dependencies and Installation

- Pytorch >= 1.7.0
- CUDA >= 11.0
```
# git clone this repository
git clone https://github.com/YuzheZhang-1999/DiffTSR
cd DiffTSR

# create new anaconda env
conda env create -f environment.yaml
conda activate DiffTSR
```
### Download the checkpoint
Please download the checkpoint file from the URL below to the ./ckpt/ folder.

- [[GoogleDrive](https://drive.google.com/drive/folders/1K6k5ZcvF3w-1MDN_gXQTdsLgFZ2SM8qy?usp=drive_link)] 

- [[BaiduDisk](https://pan.baidu.com/s/1hfaQzIp_V6H8AhAq5dfr8A)] [Password: vk9n] 


### Inference
```
python inference_DiffTSR.py
# check the code for more detail
```

### Training
```
# cd DiffTSR/train/README_train.md
# check the README_train for training details
# Please note that you need to carefully review the training sh file and the configuration yaml. Some of the configurations need to be modified according to your data or address. 
```


## ğŸ” Overview of DiffTSR
![DiffTSR](Repo_image/paper-DiffTSR-model.jpg)
### Abstract
Recovering degraded low-resolution text images is challenging, especially for Chinese text images with complex strokes and severe degradation in real-world scenarios.
Ensuring both text fidelity and style realness is crucial for high-quality text image super-resolution.
Recently, diffusion models have achieved great success in natural image synthesis and restoration due to their powerful data distribution modeling abilities and data generation capabilities
In this work, we propose an Image Diffusion Model (IDM) to restore text images with realistic styles.
For diffusion models, they are not only suitable for modeling realistic image distribution but also appropriate for learning text distribution.
Since text prior is important to guarantee the correctness of the restored text structure according to existing arts, we also propose a Text Diffusion Model (TDM) for text recognition which can guide IDM to generate text images with correct structures.
We further propose a Mixture of  Multi-modality module (MoM) to make these two diffusion models cooperate with each other in all the diffusion steps.
Extensive experiments on synthetic and real-world datasets demonstrate that our Diffusion-based Blind Text Image Super-Resolution (DiffTSR) can restore text images with more accurate text structures as well as more realistic appearances simultaneously.

### Visual performance comparison overview 
![DiffTSR](Repo_image/paper-fig1.jpg)
Blind text image super-resolution results between different methods on synthetic and real-world text images. Our method can restore text images with high text fidelity and style realness under complex strokes, severe degradation, and various text styles.


<details>
  <summary>ğŸ“· More Visual Results</summary>

  ## ![DiffTSR](Repo_image/paper-visual-comp-1.jpg)
  ## ![DiffTSR](Repo_image/paper-visual-comp-2.jpg)
  ## ![DiffTSR](Repo_image/paper-visual-comp-3.jpg)
  ## ![DiffTSR](Repo_image/paper-visual-comp-4.jpg)
  ## ![DiffTSR](Repo_image/paper-visual-comp-5.jpg)
  ## ![DiffTSR](Repo_image/paper-visual-comp-6.jpg)

</details>


## ğŸ“Citations
```
@inproceedings{zhang2024diffusion,
  title={Diffusion-based Blind Text Image Super-Resolution},
  author={Zhang, Yuzhe and Zhang, Jiawei and Li, Hao and Wang, Zhouxia and Hou, Luwei and Zou, Dongqing and Bian, Liheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={25827--25836},
  year={2024}
}
```

## ğŸ« License
This project is released under the [Apache 2.0 license](LICENSE).

## Acknowledgement
Thanks to these awesome workï¼š
- [Latent Diffusion](https://github.com/CompVis/latent-diffusion)
- [Benchmarking Chinese Text Recognition](https://github.com/FudanVI/benchmarking-chinese-text-recognition)

<details>
<summary>Statistics</summary>

![visitors](https://visitor-badge.laobi.icu/badge?page_id=YuzheZhang-1999/DiffTSR)

</details>