model:
  target: model.DiffTSR.DiffTSR_pipline

  params:
    max_length: 24
    num_classes: 6736
    transformer_dim: 768

    IDM_Unet_config:
      target: model.IDM.modules.diffusionmodules.openaimodel.UNetModel
      params:
        image_size: [32, 128]
        in_channels: 6
        out_channels: 3
        model_channels: 320
        attention_resolutions:
        - 8
        - 4
        - 2
        num_res_blocks: 2
        channel_mult:
        - 1
        - 2
        - 3
        - 4
        num_heads: 8
        resblock_updown: true
        use_spatial_transformer: true
        transformer_depth: 1
        context_dim: 160
        use_checkpoint: true
        legacy: False

    TDM_Decoder_config:
      target: model.TDM.models.modules.DynamicsTransformer
      params:
        num_classes: 6736
        transformer_dim: 768
        transformer_heads: 16
        transformer_depth: 12
        transformer_blocks: 1
        max_seq_len: 24
        diffusion_steps: 1000
        attn_layer_dropout: 0.0
        transformer_local_heads: 8
        transformer_local_size: 4
        transformer_reversible: False
        receives_context: True  

    MoM_module_config:
        target: model.MoM.MoM.MoM_model
        params:
          MoM_Unet:
            target: model.IDM.modules.diffusionmodules.openaimodel.UNetModel
            params:
              image_size: [32, 128]
              in_channels: 6
              out_channels: 3
              model_channels: 32
              attention_resolutions:
              - 0
              num_res_blocks: 2
              channel_mult:
              - 1
              - 2
              num_heads: 4
              resblock_updown: true
              use_spatial_transformer: true
              transformer_depth: 1
              context_dim: 128
              use_checkpoint: true
              legacy: False
          MoM_Transformer:
            target: model.IDM.modules.encoders.modules.BERTEmbedder
            params:
              n_embed: 160
              n_layer: 4
              vocab_size: 6736
              max_seq_len: 24
              use_tokenizer: False

    VAE_model_config:
      target: model.IDM.models.autoencoder.AutoencoderKL
      params:
        embed_dim: 3
        ddconfig:
          double_z: true
          z_channels: 3
          resolution: 512
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0
    
    Text_Prediction_config:
      target: model.TDM.models.text_prediction.Text_Prediction